# Code for joining tables in lakehouse
insight_new = insight_final.join(
    dis_df.select("DisbursementsId", "TotalAmount"),
    on="DisbursementsId",
    how="left"
)

from pyspark.sql.functions import count, when
 
party_type_by_zip = df.groupBy("Zip").agg(
    count("*").alias("total_disbursements"),
    count(when(col("DisbursementPartyType") == "Single", True)).alias("single_party_count"),
    count(when(col("DisbursementPartyType") == "Multi", True)).alias("multi_party_count")
)

from pyspark.sql import functions as F

 
state_dis_count = insight_final.groupBy("state").agg(

    F.countDistinct("SanitizedName").alias("StateApartmentCount")
)
from pyspark.sql.functions import col, count, when, round
 
# Group and count total, expired, and completed disbursements per ZIP
zip_stats_df = disbursements_with_zip.groupBy("Zip").agg(
    count("*").alias("total_disbursements"),
    count(when(col("DisbursementStatusName") == "Expired", True)).alias("expired_count"),
    count(when(col("DisbursementStatusName") == "Complete", True)).alias("completed_count")
)
 
# Calculate rates
zip_rates_df = zip_stats_df \
    .withColumn("expiration_rate", round(col("expired_count") / col("total_disbursements"), 3)) \
    .withColumn("completion_rate", round(col("completed_count") / col("total_disbursements"), 3))
 
# Show the result
display(zip_rates_df)

city_dis_count = insight_final.groupBy("city").agg(

    F.countDistinct("SanitizedName").alias("CityApartmentCount")
)

disbursements_with_zip_df = spark.read.table("disbursements_with_zip")
zip_rates_df = spark.read.table("zip_rates")
 
# Perform the join on "Zip"
disbursements_with_rates = disbursements_with_zip_df.join(
    zip_rates_df,
    on="Zip",
    how="left"
)
 
# Preview the joined result
display(disbursements_with_rates)
